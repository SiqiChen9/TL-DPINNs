{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navier Stokes in JAX\n",
    "\n",
    "$\\begin{aligned}\n",
    "\t&w_t+\\boldsymbol{u}\\cdot\\nabla w=\\frac{1}{\\mathrm{Re}}\\Delta w,\\quad in[0,T]\\times \\Omega, \\\\\n",
    "\t&\\nabla \\cdot \\boldsymbol{u}=0,\\quad in[0,T]\\times \\Omega, \\\\\n",
    "\t&w(0,x,y)=w_0(x,y),\\quad in\\Omega, \\\\\n",
    "\t&\\Omega=[0,2\\pi]^2, Re=100\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgments\n",
    "\n",
    "The following sections contain code that is based on the original work by Sifan Wang and Paris Perdikaris. \n",
    "\n",
    "The original code can be found at https://github.com/PredictiveIntelligenceLab/CausalPINNs/tree/main. \n",
    "\n",
    "We have made some modifications to the original code to suit the needs of our study and to allow for a fair comparison with our method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "import jax.numpy as np\n",
    "from jax import random, grad, vmap, jit, jacfwd, jacrev\n",
    "from jax.example_libraries import optimizers\n",
    "from jax.experimental.ode import odeint\n",
    "from jax.nn import relu\n",
    "from jax.config import config\n",
    "from jax import lax\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import itertools\n",
    "from functools import partial\n",
    "from torch.utils import data\n",
    "from tqdm import trange\n",
    "\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.linalg import lstsq\n",
    "from scipy.optimize import lsq_linear\n",
    "# from sklearn.linear_model import RidgeCV\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "from scipy.optimize import least_squares\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Define the neural net\n",
    "def init_layer(key, d_in, d_out):\n",
    "    k1, k2 = random.split(key)\n",
    "    glorot_stddev = 1. / np.sqrt((d_in + d_out) / 2.)\n",
    "    W = glorot_stddev * random.normal(k1, (d_in, d_out))\n",
    "    b = np.zeros(d_out)\n",
    "    return W, b\n",
    "\n",
    "\n",
    "def MLP(layers, activation=relu):\n",
    "    ''' Vanilla MLP'''\n",
    "\n",
    "    def init(rng_key):\n",
    "        key, *keys = random.split(rng_key, len(layers))\n",
    "        params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
    "        return params\n",
    "\n",
    "    def apply(params, inputs):\n",
    "        for W, b in params[:-1]:\n",
    "            outputs = np.dot(inputs, W) + b\n",
    "            inputs = activation(outputs)\n",
    "        W, b = params[-1]\n",
    "        outputs = np.dot(inputs, W) + b\n",
    "        return outputs\n",
    "\n",
    "    return init, apply\n",
    "\n",
    "\n",
    "# Define the neural net\n",
    "def modified_MLP_II(layers, L_x=1.0, L_y=1.0, M_t=1, M_x=1, M_y=1, activation=relu):\n",
    "    def xavier_init(key, d_in, d_out):\n",
    "        glorot_stddev = 1. / np.sqrt((d_in + d_out) / 2.)\n",
    "        W = glorot_stddev * random.normal(key, (d_in, d_out))\n",
    "        b = np.zeros(d_out)\n",
    "        return W, b\n",
    "\n",
    "    w_x = 2.0 * np.pi / L_x\n",
    "    w_y = 2.0 * np.pi / L_y\n",
    "    k_x = np.arange(1, M_x + 1)\n",
    "    k_y = np.arange(1, M_y + 1)\n",
    "    k_xx, k_yy = np.meshgrid(k_x, k_y)\n",
    "    k_xx = k_xx.flatten()\n",
    "    k_yy = k_yy.flatten()\n",
    "\n",
    "    # Define input encoding function\n",
    "    def input_encoding(t, x, y):\n",
    "        k_t = np.power(10.0, np.arange(0, M_t + 1))\n",
    "        out = np.hstack([1, k_t * t,\n",
    "                         np.cos(k_x * w_x * x), np.cos(k_y * w_y * y),\n",
    "                         np.sin(k_x * w_x * x), np.sin(k_y * w_y * y),\n",
    "                         np.cos(k_xx * w_x * x) * np.cos(k_yy * w_y * y),\n",
    "                         np.cos(k_xx * w_x * x) * np.sin(k_yy * w_y * y),\n",
    "                         np.sin(k_xx * w_x * x) * np.cos(k_yy * w_y * y),\n",
    "                         np.sin(k_xx * w_x * x) * np.sin(k_yy * w_y * y)])\n",
    "        return out\n",
    "\n",
    "    def init(rng_key):\n",
    "        U1, b1 = xavier_init(random.PRNGKey(12345), layers[0], layers[1])\n",
    "        U2, b2 = xavier_init(random.PRNGKey(54321), layers[0], layers[1])\n",
    "\n",
    "        def init_layer(key, d_in, d_out):\n",
    "            k1, k2 = random.split(key)\n",
    "            W, b = xavier_init(k1, d_in, d_out)\n",
    "            return W, b\n",
    "\n",
    "        key, *keys = random.split(rng_key, len(layers))\n",
    "        params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
    "        return (params, U1, b1, U2, b2)\n",
    "\n",
    "    def apply(params, inputs):\n",
    "        params, U1, b1, U2, b2 = params\n",
    "\n",
    "        t = inputs[0]\n",
    "        x = inputs[1]\n",
    "        y = inputs[2]\n",
    "        inputs = input_encoding(t, x, y)\n",
    "        U = activation(np.dot(inputs, U1) + b1)\n",
    "        V = activation(np.dot(inputs, U2) + b2)\n",
    "        for W, b in params[:-1]:\n",
    "            outputs = activation(np.dot(inputs, W) + b)\n",
    "            inputs = np.multiply(outputs, U) + np.multiply(1 - outputs, V)\n",
    "        W, b = params[-1]\n",
    "        outputs = np.dot(inputs, W) + b\n",
    "        return outputs\n",
    "\n",
    "    return init, apply\n",
    "\n",
    "\n",
    "# Define the neural net\n",
    "def modified_MLP_III(layers, L_x=1.0, L_y=1.0, M_t=1, M_x=1, M_y=1, activation=relu):\n",
    "    def xavier_init(key, d_in, d_out):\n",
    "        glorot_stddev = 1. / np.sqrt((d_in + d_out) / 2.)\n",
    "        W = glorot_stddev * random.normal(key, (d_in, d_out))\n",
    "        b = np.zeros(d_out)\n",
    "        return W, b\n",
    "\n",
    "    w_x = 2.0 * np.pi / L_x\n",
    "    w_y = 2.0 * np.pi / L_y\n",
    "    k_x = np.arange(1, M_x + 1)\n",
    "    k_y = np.arange(1, M_y + 1)\n",
    "    k_xx, k_yy = np.meshgrid(k_x, k_y)\n",
    "    k_xx = k_xx.flatten()\n",
    "    k_yy = k_yy.flatten()\n",
    "\n",
    "    # Define input encoding function\n",
    "    def spatial_encoding(x, y, M_x, M_y):\n",
    "        out = np.hstack([1, np.cos(k_x * w_x * x), np.cos(k_y * w_y * y),\n",
    "                         np.sin(k_x * w_x * x), np.sin(k_y * w_y * y),\n",
    "                         np.cos(k_xx * w_x * x) * np.cos(k_yy * w_y * y),\n",
    "                         np.cos(k_xx * w_x * x) * np.sin(k_yy * w_y * y),\n",
    "                         np.sin(k_xx * w_x * x) * np.cos(k_yy * w_y * y),\n",
    "                         np.sin(k_xx * w_x * x) * np.sin(k_yy * w_y * y)])\n",
    "        return out\n",
    "\n",
    "    def temporal_encoding(t, M_t):\n",
    "        k = np.power(10.0, np.arange(0, M_t + 1))\n",
    "        out = k * t\n",
    "        return out\n",
    "\n",
    "    def init(rng_key):\n",
    "        U1, b1 = xavier_init(random.PRNGKey(12345), M_t + 1, layers[1])\n",
    "        U2, b2 = xavier_init(random.PRNGKey(54321), 2 * M_x + 2 * M_y + 4 * M_x * M_y + 1, layers[1])\n",
    "\n",
    "        def init_layer(key, d_in, d_out):\n",
    "            k1, k2 = random.split(key)\n",
    "            W, b = xavier_init(k1, d_in, d_out)\n",
    "            return W, b\n",
    "\n",
    "        key, *keys = random.split(rng_key, len(layers))\n",
    "        params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
    "        return (params, U1, b1, U2, b2)\n",
    "\n",
    "    def apply(params, inputs):\n",
    "        params, U1, b1, U2, b2 = params\n",
    "        t = inputs[0]\n",
    "        x = inputs[1]\n",
    "        y = inputs[2]\n",
    "        H_t = temporal_encoding(t, M_t)\n",
    "        H_x = spatial_encoding(x, y, M_x, M_y)\n",
    "        inputs = np.hstack([H_t, H_x])\n",
    "        U = activation(np.dot(H_t, U1) + b1)\n",
    "        V = activation(np.dot(H_x, U2) + b2)\n",
    "        for W, b in params[:-1]:\n",
    "            outputs = activation(np.dot(inputs, W) + b)\n",
    "            inputs = np.multiply(outputs, U) + np.multiply(1 - outputs, V)\n",
    "        W, b = params[-1]\n",
    "        outputs = np.dot(inputs, W) + b\n",
    "        return outputs\n",
    "\n",
    "    return init, apply\n",
    "\n",
    "\n",
    "class DataGenerator(data.Dataset):\n",
    "    def __init__(self, t0, t1, n_t=10, n_x=64, rng_key=random.PRNGKey(1234)):\n",
    "        'Initialization'\n",
    "        self.t0 = t0\n",
    "        self.t1 = t1 + 0.01 * t1\n",
    "        self.n_t = n_t\n",
    "        self.n_x = n_x\n",
    "        self.key = rng_key\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        batch = self.__data_generation(subkey)\n",
    "        return batch\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def __data_generation(self, key):\n",
    "        'Generates data containing batch_size samples'\n",
    "        subkeys = random.split(key, 2)\n",
    "        t_r = random.uniform(subkeys[0], shape=(self.n_t,), minval=self.t0, maxval=self.t1).sort()\n",
    "        x_r = random.uniform(subkeys[1], shape=(self.n_x, 2), minval=0.0, maxval=2.0 * np.pi)\n",
    "        batch = (t_r, x_r)\n",
    "        return batch\n",
    "\n",
    "\n",
    "# Define the model\n",
    "class PINN:\n",
    "    def __init__(self, key, w_exact, layers, M_t, M_x, M_y, state0, t0, t1, n_t, x_star, y_star, tol):\n",
    "\n",
    "        self.w_exact = w_exact\n",
    "\n",
    "        self.M_t = M_t\n",
    "        self.M_x = M_x\n",
    "        self.M_y = M_y\n",
    "\n",
    "        # grid\n",
    "        self.n_t = n_t\n",
    "        self.t0 = t0\n",
    "        self.t1 = t1\n",
    "        eps = 0.01 * t1\n",
    "        self.t = np.linspace(self.t0, self.t1 + eps, n_t)\n",
    "        self.x_star = x_star\n",
    "        self.y_star = y_star\n",
    "\n",
    "        # initial state\n",
    "        self.state0 = state0\n",
    "\n",
    "        self.tol = tol\n",
    "        self.M = np.triu(np.ones((n_t, n_t)), k=1).T\n",
    "\n",
    "        self.init, self.apply = modified_MLP_II(layers, L_x=2 * np.pi, L_y=2 * np.pi, M_t=M_t, M_x=M_x, M_y=M_y,\n",
    "                                                activation=np.tanh)\n",
    "        params = self.init(rng_key=key)\n",
    "\n",
    "        # Use optimizers to set optimizer initialization and update functions\n",
    "        self.opt_init, self.opt_update, self.get_params = optimizers.adam(optimizers.exponential_decay(1e-3,\n",
    "                                                                                                       decay_steps=10000,\n",
    "                                                                                                       decay_rate=0.9))\n",
    "        self.opt_state = self.opt_init(params)\n",
    "        _, self.unravel = ravel_pytree(params)\n",
    "\n",
    "        self.u0_pred_fn = vmap(vmap(self.u_net, (None, None, None, 0)), (None, None, 0, None))\n",
    "        self.v0_pred_fn = vmap(vmap(self.v_net, (None, None, None, 0)), (None, None, 0, None))\n",
    "        self.w0_pred_fn = vmap(vmap(self.vorticity_net, (None, None, None, 0)), (None, None, 0, None))\n",
    "        self.u_pred_fn = vmap(vmap(vmap(self.u_net, (None, None, None, 0)), (None, None, 0, None)),\n",
    "                              (None, 0, None, None))\n",
    "        self.v_pred_fn = vmap(vmap(vmap(self.v_net, (None, None, None, 0)), (None, None, 0, None)),\n",
    "                              (None, 0, None, None))\n",
    "        self.w_pred_fn = vmap(vmap(vmap(self.vorticity_net, (None, None, None, 0)), (None, None, 0, None)),\n",
    "                              (None, 0, None, None))\n",
    "        # self.r_pred_fn = vmap(vmap(vmap(self.residual_net, (None, None, None, 0)), (None, None, 0, None)), (None, 0, None, None))\n",
    "        self.r_pred_fn = vmap(vmap(self.residual_net, (None, None, 0, 0)), (None, 0, None, None))\n",
    "\n",
    "        # Logger\n",
    "        self.itercount = itertools.count()\n",
    "\n",
    "        self.loss_log = []\n",
    "        self.loss_ics_log = []\n",
    "        self.loss_u0_log = []\n",
    "        self.loss_v0_log = []\n",
    "        self.loss_w0_log = []\n",
    "        self.loss_bcs_log = []\n",
    "        self.loss_res_w_log = []\n",
    "        self.loss_res_c_log = []\n",
    "        self.l2_error_log = []\n",
    "\n",
    "    def neural_net(self, params, t, x, y):\n",
    "        z = np.stack([t, x, y])\n",
    "        outputs = self.apply(params, z)\n",
    "        u = outputs[0]\n",
    "        v = outputs[1]\n",
    "        return u, v\n",
    "\n",
    "    def u_net(self, params, t, x, y):\n",
    "        u, _ = self.neural_net(params, t, x, y)\n",
    "        return u\n",
    "\n",
    "    def v_net(self, params, t, x, y):\n",
    "        _, v = self.neural_net(params, t, x, y)\n",
    "        return v\n",
    "\n",
    "    def vorticity_net(self, params, t, x, y):\n",
    "        u_y = grad(self.u_net, argnums=3)(params, t, x, y)\n",
    "        v_x = grad(self.v_net, argnums=2)(params, t, x, y)\n",
    "        w = v_x - u_y\n",
    "        return w\n",
    "\n",
    "    def residual_net(self, params, t, x, y):\n",
    "\n",
    "        u, v = self.neural_net(params, t, x, y)\n",
    "\n",
    "        u_x = grad(self.u_net, argnums=2)(params, t, x, y)\n",
    "        v_y = grad(self.v_net, argnums=3)(params, t, x, y)\n",
    "\n",
    "        w_t = grad(self.vorticity_net, argnums=1)(params, t, x, y)\n",
    "        w_x = grad(self.vorticity_net, argnums=2)(params, t, x, y)\n",
    "        w_y = grad(self.vorticity_net, argnums=3)(params, t, x, y)\n",
    "\n",
    "        w_xx = grad(grad(self.vorticity_net, argnums=2), argnums=2)(params, t, x, y)\n",
    "        w_yy = grad(grad(self.vorticity_net, argnums=3), argnums=3)(params, t, x, y)\n",
    "\n",
    "        res_w = w_t + u * w_x + v * w_y - nu * (w_xx + w_yy)\n",
    "        res_c = u_x + v_y\n",
    "\n",
    "        return res_w, res_c\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def residuals_and_weights(self, params, tol, batch):\n",
    "        t_r, x_r = batch\n",
    "        loss_u0, loss_v0, loss_w0 = self.loss_ics(params)\n",
    "        L_0 = 1e5 * (loss_u0 + loss_v0 + loss_w0)\n",
    "        res_w_pred, res_c_pred = self.r_pred_fn(params, t_r, x_r[:, 0], x_r[:, 1])\n",
    "        L_t = np.mean(res_w_pred ** 2 + 100 * res_c_pred ** 2, axis=1)\n",
    "        W = lax.stop_gradient(np.exp(- tol * (self.M @ L_t + L_0)))\n",
    "        return L_0, L_t, W\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_ics(self, params):\n",
    "        # Compute forward pass\n",
    "        u0_pred = self.u0_pred_fn(params, 0.0, self.x_star, self.y_star)\n",
    "        v0_pred = self.v0_pred_fn(params, 0.0, self.x_star, self.y_star)\n",
    "        w0_pred = self.w0_pred_fn(params, 0.0, self.x_star, self.y_star)\n",
    "        # Compute loss\n",
    "        loss_u0 = np.mean((u0_pred - self.state0[0, :, :]) ** 2)\n",
    "        loss_v0 = np.mean((v0_pred - self.state0[1, :, :]) ** 2)\n",
    "        loss_w0 = np.mean((w0_pred - self.state0[2, :, :]) ** 2)\n",
    "        return loss_u0, loss_v0, loss_w0\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_res(self, params, batch):\n",
    "        t_r, x_r = batch\n",
    "        # Compute forward pass\n",
    "        res_w_pred, res_c_pred = self.r_pred_fn(params, t_r, x_r[:, 0], x_r[:, 1])\n",
    "        # Compute loss\n",
    "        loss_res_w = np.mean(res_w_pred ** 2)\n",
    "        loss_res_c = np.mean(res_c_pred ** 2)\n",
    "        return loss_res_w, loss_res_c\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss(self, params, batch):\n",
    "\n",
    "        L_0, L_t, W = self.residuals_and_weights(params, self.tol, batch)\n",
    "        # Compute loss\n",
    "        loss = np.mean(W * L_t + L_0)\n",
    "        return loss\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def compute_l2_error(self, params):\n",
    "        w_pred = self.w_pred_fn(params, t_star[:num_step], x_star, y_star)\n",
    "        l2_error = np.linalg.norm(w_pred - self.w_exact) / np.linalg.norm(self.w_exact)\n",
    "        return l2_error\n",
    "\n",
    "    # Define a compiled update step\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def step(self, i, opt_state, batch):\n",
    "        params = self.get_params(opt_state)\n",
    "        g = grad(self.loss)(params, batch)\n",
    "        return self.opt_update(i, g, opt_state)\n",
    "\n",
    "    # Optimize parameters in a loop\n",
    "    def train(self, dataset, nIter=10000):\n",
    "        res_data = iter(dataset)\n",
    "        pbar = trange(nIter)\n",
    "        # Main training loop\n",
    "        for it in pbar:\n",
    "            batch = next(res_data)\n",
    "            self.current_count = next(self.itercount)\n",
    "            self.opt_state = self.step(self.current_count, self.opt_state, batch)\n",
    "\n",
    "            if it % 1000 == 0:\n",
    "                params = self.get_params(self.opt_state)\n",
    "\n",
    "                l2_error_value = self.compute_l2_error(params)\n",
    "\n",
    "                loss_value = self.loss(params, batch)\n",
    "\n",
    "                loss_u0_value, loss_v0_value, loss_w0_value = self.loss_ics(params)\n",
    "                loss_res_w_value, loss_res_c_value = self.loss_res(params, batch)\n",
    "                _, _, W_value = self.residuals_and_weights(params, tol, batch)\n",
    "\n",
    "                self.l2_error_log.append(l2_error_value)\n",
    "                self.loss_log.append(loss_value)\n",
    "                self.loss_u0_log.append(loss_u0_value)\n",
    "                self.loss_v0_log.append(loss_v0_value)\n",
    "                self.loss_w0_log.append(loss_w0_value)\n",
    "                self.loss_res_w_log.append(loss_res_w_value)\n",
    "                self.loss_res_c_log.append(loss_res_c_value)\n",
    "\n",
    "                pbar.set_postfix({'l2 error': l2_error_value,\n",
    "                                  'Loss': loss_value,\n",
    "                                  'loss_u0': loss_u0_value,\n",
    "                                  'loss_v0': loss_v0_value,\n",
    "                                  'loss_w0': loss_w0_value,\n",
    "                                  'loss_res_w': loss_res_w_value,\n",
    "                                  'loss_res_c': loss_res_c_value,\n",
    "                                  'W_min': W_value.min()})\n",
    "\n",
    "                if W_value.min() > 0.99:\n",
    "                    break\n",
    "\n",
    "\n",
    "data = np.load('NS.npy', allow_pickle=True).item()\n",
    "# Test data\n",
    "sol = data['sol']\n",
    "\n",
    "t_star = data['t']\n",
    "x_star = data['x']\n",
    "y_star = data['y']\n",
    "nu = data['viscosity']\n",
    "\n",
    "# downsampling\n",
    "sol = sol\n",
    "x_star = x_star\n",
    "y_star = y_star\n",
    "\n",
    "# Create PINNs model\n",
    "key = random.PRNGKey(1234)\n",
    "\n",
    "u0 = data['u0']\n",
    "v0 = data['v0']\n",
    "w0 = data['w0']\n",
    "state0 = np.stack([u0, v0, w0])\n",
    "M_t = 2\n",
    "M_x = 5\n",
    "M_y = 5\n",
    "d0 = 2 * M_x + 2 * M_y + 4 * M_x * M_y + M_t + 2\n",
    "layers = [d0, 128, 128, 128, 128, 2]\n",
    "\n",
    "num_step = 10\n",
    "t0 = 0.0\n",
    "t1 = t_star[num_step]\n",
    "n_t = 32\n",
    "tol = 1.0\n",
    "tol_list = [1e-3, 1e-2, 1e-1, 1e0]\n",
    "# tol_list = [1e0]\n",
    "\n",
    "\n",
    "# Create data set\n",
    "n_x = 256\n",
    "dataset = DataGenerator(t0, t1, n_t, n_x)\n",
    "\n",
    "N = 20\n",
    "w_pred_list = []\n",
    "params_list = []\n",
    "losses_list = []\n",
    "\n",
    "path='result_causal'\n",
    "\n",
    "for k in range(N):\n",
    "    # Initialize model\n",
    "    print('Final Time: {}'.format(k + 1))\n",
    "    w_exact = sol[num_step * k: num_step * (k + 1), :, :]\n",
    "    model = PINN(key, w_exact, layers, M_t, M_x, M_y, state0, t0, t1, n_t, x_star, y_star, tol)\n",
    "\n",
    "    # Train\n",
    "    for tol in tol_list:\n",
    "        model.tol = tol\n",
    "        print('tol:', model.tol)\n",
    "        # Train\n",
    "        model.train(dataset, nIter=100000)\n",
    "\n",
    "    # Store\n",
    "    params = model.get_params(model.opt_state)\n",
    "    w_pred = model.w_pred_fn(params, t_star[:num_step], x_star, y_star)\n",
    "    w_pred_list.append(w_pred)\n",
    "    flat_params, _ = ravel_pytree(params)\n",
    "    params_list.append(flat_params)\n",
    "    losses_list.append([model.l2_error_log,\n",
    "                        model.loss_log,\n",
    "                        model.loss_u0_log,\n",
    "                        model.loss_v0_log,\n",
    "                        model.loss_w0_log,\n",
    "                        model.loss_res_w_log,\n",
    "                        model.loss_res_c_log, ])\n",
    "\n",
    "#     np.save('causal_w_pred_list.npy', w_pred_list)\n",
    "#     np.save('causal_params_list.npy', params_list)\n",
    "#     np.save('causal_losses_list.npy', losses_list)\n",
    "    pickle.dump(w_pred_list, open(f'{path}/causal_w_pred_list.pkl', 'wb'))\n",
    "    pickle.dump(params_list, open(f'{path}/causal_params_list.pkl', 'wb'))\n",
    "    pickle.dump(losses_list, open(f'{path}/causal_losses_list.pkl', 'wb'))\n",
    "\n",
    "    # error\n",
    "    w_preds = np.vstack(w_pred_list)\n",
    "    error = np.linalg.norm(w_preds - sol[:num_step * (k + 1), :, :]) / np.linalg.norm(sol[:num_step * (k + 1), :, :])\n",
    "    print('Relative l2 error: {:.3e}'.format(error))\n",
    "\n",
    "    params = model.get_params(model.opt_state)\n",
    "    u0_pred = model.u0_pred_fn(params, t1, x_star, y_star)\n",
    "    v0_pred = model.v0_pred_fn(params, t1, x_star, y_star)\n",
    "    w0_pred = model.w0_pred_fn(params, t1, x_star, y_star)\n",
    "    state0 = np.stack([u0_pred, v0_pred, w0_pred])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
