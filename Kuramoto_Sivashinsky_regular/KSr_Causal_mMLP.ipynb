{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kuramoto Sivashinsky (regular) equation in JAX\n",
    "\n",
    "$\\begin{aligned}\n",
    "    &u_t + \\alpha uu_x + \\beta u_{xx} + \\gamma u_{xxxx} = 0 \\\\\n",
    "    &\\alpha = 5, \\beta = 0.5, \\gamma = 0.005 \\\\\n",
    "    &u_0(x) = -\\sin (\\pi x) \n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgments\n",
    "\n",
    "The following sections contain code that is based on the original work by Sifan Wang and Paris Perdikaris. \n",
    "\n",
    "The original code can be found at https://github.com/PredictiveIntelligenceLab/CausalPINNs/tree/main. \n",
    "\n",
    "We have made some modifications to the original code to suit the needs of our study and to allow for a fair comparison with our method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vG60QJLoPwj4"
   },
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "import jax.numpy as np\n",
    "from jax import random, grad, vmap, jit, jacfwd, jacrev\n",
    "from jax.example_libraries import optimizers\n",
    "from jax.experimental.ode import odeint\n",
    "from jax.experimental.jet import jet\n",
    "from jax.nn import relu\n",
    "from jax.config import config\n",
    "from jax import lax\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import itertools\n",
    "from functools import partial\n",
    "from torch.utils import data\n",
    "from tqdm import trange\n",
    "\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NZ_kHHekUfP4"
   },
   "outputs": [],
   "source": [
    "# Define MLP\n",
    "def MLP(layers, L=1.0, M=1, activation=relu):\n",
    "  # Define input encoding function\n",
    "    def input_encoding(t, x):\n",
    "        w = 2.0 * np.pi / L\n",
    "        k = np.arange(1, M + 1)\n",
    "        out = np.hstack([t, 1, \n",
    "                         np.cos(k * w * x), np.sin(k * w * x)])\n",
    "        return out\n",
    "   \n",
    "    def init(rng_key):\n",
    "      def init_layer(key, d_in, d_out):\n",
    "          k1, k2 = random.split(key)\n",
    "          glorot_stddev = 1.0 / np.sqrt((d_in + d_out) / 2.)\n",
    "          W = glorot_stddev * random.normal(k1, (d_in, d_out))\n",
    "          b = np.zeros(d_out)\n",
    "          return W, b\n",
    "      key, *keys = random.split(rng_key, len(layers))\n",
    "      params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
    "      return params\n",
    "    def apply(params, inputs):\n",
    "        t = inputs[0]\n",
    "        x = inputs[1]\n",
    "        H = input_encoding(t, x)\n",
    "        for W, b in params[:-1]:\n",
    "            outputs = np.dot(H, W) + b\n",
    "            H = activation(outputs)\n",
    "        W, b = params[-1]\n",
    "        outputs = np.dot(H, W) + b\n",
    "        return outputs\n",
    "    return init, apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define modified MLP\n",
    "def modified_MLP(layers, L=1.0, M=1, activation=relu):\n",
    "  def xavier_init(key, d_in, d_out):\n",
    "      glorot_stddev = 1. / np.sqrt((d_in + d_out) / 2.)\n",
    "      W = glorot_stddev * random.normal(key, (d_in, d_out))\n",
    "      b = np.zeros(d_out)\n",
    "      return W, b\n",
    "\n",
    "  # Define input encoding function\n",
    "  def input_encoding(t, x):\n",
    "      w = 2 * np.pi / L\n",
    "      k = np.arange(1, M + 1)\n",
    "      out = np.hstack([t, 1, \n",
    "                         np.cos(k * w * x), np.sin(k * w * x)])\n",
    "      return out\n",
    "\n",
    "\n",
    "  def init(rng_key):\n",
    "      U1, b1 =  xavier_init(random.PRNGKey(12345), layers[0], layers[1])\n",
    "      U2, b2 =  xavier_init(random.PRNGKey(54321), layers[0], layers[1])\n",
    "      def init_layer(key, d_in, d_out):\n",
    "          k1, k2 = random.split(key)\n",
    "          W, b = xavier_init(k1, d_in, d_out)\n",
    "          return W, b\n",
    "      key, *keys = random.split(rng_key, len(layers))\n",
    "      params = list(map(init_layer, keys, layers[:-1], layers[1:]))\n",
    "      return (params, U1, b1, U2, b2) \n",
    "\n",
    "  def apply(params, inputs):\n",
    "      params, U1, b1, U2, b2 = params\n",
    "        \n",
    "      t = inputs[0]\n",
    "      x = inputs[1]\n",
    "      inputs = input_encoding(t, x)  \n",
    "      U = activation(np.dot(inputs, U1) + b1)\n",
    "      V = activation(np.dot(inputs, U2) + b2)\n",
    "      for W, b in params[:-1]:\n",
    "          outputs = activation(np.dot(inputs, W) + b)\n",
    "          inputs = np.multiply(outputs, U) + np.multiply(1 - outputs, V) \n",
    "      W, b = params[-1]\n",
    "      outputs = np.dot(inputs, W) + b\n",
    "      return outputs\n",
    "  return init, apply     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(data.Dataset):\n",
    "    def __init__(self, t0, t1, n_t=10, n_x=64, rng_key=random.PRNGKey(1234)):\n",
    "        'Initialization'\n",
    "        self.t0 = t0\n",
    "        self.t1 = t1\n",
    "        self.n_t = n_t\n",
    "        self.n_x = n_x\n",
    "        self.key = rng_key\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        batch = self.__data_generation(subkey)\n",
    "        return batch\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def __data_generation(self, key):\n",
    "        'Generates data containing batch_size samples'\n",
    "        subkeys = random.split(key, 2)\n",
    "        t_r = random.uniform(subkeys[0], shape=(self.n_t,), minval=self.t0, maxval=self.t1).sort()\n",
    "        x_r = random.uniform(subkeys[1], shape=(self.n_x,), minval=-1.0, maxval=1.0)\n",
    "        batch = (t_r, x_r)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AH0KddsTid1o"
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class PINN:\n",
    "    def __init__(self, key, arch, layers, M_x, state0, t0, t1, n_t, n_x, tol=1.0): \n",
    "        \n",
    "        # grid\n",
    "        eps = 0.01 * t1\n",
    "        self.t_r = np.linspace(t0, t1 + eps, n_t)\n",
    "        self.x_r = np.linspace(-1.0, 1.0, n_x)\n",
    "\n",
    "        # IC\n",
    "        t_ic = np.zeros((x_star.shape[0], 1))\n",
    "        x_ic = x_star.reshape(-1, 1)\n",
    "        self.X_ic = np.hstack([t_ic, x_ic])\n",
    "        self.Y_ic = state0\n",
    "    \n",
    "        # Weight matrix and causal parameter\n",
    "        self.M = np.triu(np.ones((n_t, n_t)), k=1).T \n",
    "        self.tol = tol\n",
    "              \n",
    "        if arch == 'MLP':\n",
    "            d0 = 2 * M_x + 2\n",
    "            layers = [d0] + layers\n",
    "            self.init, self.apply = MLP(layers, L=2.0, M=M_x, activation=np.tanh)\n",
    "            params = self.init(rng_key = key)\n",
    "        \n",
    "        if arch == 'modified_MLP':\n",
    "            d0 = 2 * M_x + 2\n",
    "            layers = [d0] + layers\n",
    "            self.init, self.apply = modified_MLP(layers, L=2.0, M=M_x, activation=np.tanh)\n",
    "            params = self.init(rng_key = key)\n",
    "\n",
    "            \n",
    "        # Use optimizers to set optimizer initialization and update functions\n",
    "        lr = optimizers.exponential_decay(1e-3, decay_steps=5000, decay_rate=0.9)\n",
    "        self.opt_init,  self.opt_update, self.get_params = optimizers.adam(lr)\n",
    "        self.opt_state = self.opt_init(params) \n",
    "        _, self.unravel = ravel_pytree(params)\n",
    "        \n",
    "        # Evaluate functions over a grid\n",
    "        self.u_pred_fn = vmap(vmap(self.neural_net, (None, 0, None)), (None, None, 0))  # consistent with the dataset\n",
    "        self.r_pred_fn = vmap(vmap(self.residual_net, (None, None, 0)), (None, 0, None))\n",
    "\n",
    "        # Logger\n",
    "        self.loss_log = []\n",
    "        self.loss_ics_log = []\n",
    "        self.loss_res_log = []\n",
    "        \n",
    "        self.itercount = itertools.count()\n",
    "    \n",
    "    \n",
    "    def neural_net(self, params, t, x):\n",
    "        z = np.stack([t, x])\n",
    "        outputs = self.apply(params, z)\n",
    "        return outputs[0]\n",
    "\n",
    "    def residual_net(self, params, t, x): \n",
    "        u = self.neural_net(params, t, x)\n",
    "        u_t = grad(self.neural_net, argnums=1)(params, t, x)\n",
    "        u_fn = lambda x: self.neural_net(params, t, x) # For using Taylor-mode AD\n",
    "        _, (u_x, u_xx, u_xxx, u_xxxx) = jet(u_fn, (x, ), [[1.0, 0.0, 0.0, 0.0]]) #  Taylor-mode AD\n",
    "        return u_t + 5 * u * u_x + 0.5 * u_xx + 0.005 * u_xxxx\n",
    "    \n",
    "    # Compute the temporal weights\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def residuals_and_weights(self, params, batch, tol):\n",
    "        t_r, x_r = batch\n",
    "        L_0 = 1e3 * self.loss_ics(params)\n",
    "        r_pred = self.r_pred_fn(params, t_r, x_r)\n",
    "        L_t = np.mean(r_pred**2, axis=1)\n",
    "        W = lax.stop_gradient(np.exp(- tol * (self.M @ L_t + L_0) ))\n",
    "        return L_0, L_t, W\n",
    "\n",
    "    # Initial condition loss\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_ics(self, params):\n",
    "        # Compute forward pass\n",
    "        u_pred = vmap(self.neural_net, (None, 0, 0))(params, self.X_ic[:,0], self.X_ic[:,1])\n",
    "        # Compute loss\n",
    "        loss_ics = np.mean((self.Y_ic.flatten() - u_pred.flatten())**2)\n",
    "        return loss_ics\n",
    "\n",
    "    # Residual loss\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss_res(self, params, batch):\n",
    "        t_r, x_r = batch\n",
    "        # Compute forward pass        \n",
    "        r_pred = self.r_pred_fn(params, t_r, x_r)\n",
    "        # Compute loss\n",
    "        loss_r = np.mean(r_pred**2)\n",
    "        return loss_r  \n",
    "\n",
    "    # Total loss\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def loss(self, params, batch):\n",
    "        L_0, L_t, W = self.residuals_and_weights(params, batch, self.tol)\n",
    "        # Compute loss\n",
    "        loss = np.mean(W * L_t + L_0)\n",
    "        return loss\n",
    "\n",
    "    # Define a compiled update step\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def step(self, i, opt_state, batch):\n",
    "        params = self.get_params(opt_state)\n",
    "        g = grad(self.loss)(params, batch)\n",
    "        return self.opt_update(i, g, opt_state)\n",
    "\n",
    "    # Optimize parameters in a loop\n",
    "    def train(self, dataset, nIter = 10000):\n",
    "        res_data = iter(dataset)\n",
    "        pbar = trange(nIter)\n",
    "        # Main training loop\n",
    "        for it in pbar:\n",
    "            # Get batch\n",
    "            batch= next(res_data)\n",
    "            self.current_count = next(self.itercount)\n",
    "            self.opt_state = self.step(self.current_count, self.opt_state, batch)\n",
    "            \n",
    "            if it % 1000 == 0:\n",
    "                params = self.get_params(self.opt_state)\n",
    "\n",
    "                loss_value = self.loss(params, batch)\n",
    "                loss_ics_value = self.loss_ics(params)\n",
    "                loss_res_value = self.loss_res(params, batch)\n",
    "                _, _, W_value = self.residuals_and_weights(params, batch, self.tol)\n",
    "\n",
    "                self.loss_log.append(loss_value)\n",
    "                self.loss_ics_log.append(loss_ics_value)\n",
    "                self.loss_res_log.append(loss_res_value)\n",
    "\n",
    "                pbar.set_postfix({'Loss': loss_value, \n",
    "                                  'loss_ics' : loss_ics_value, \n",
    "                                  'loss_res':  loss_res_value,\n",
    "                                  'W_min'  : W_value.min()})\n",
    "                \n",
    "                if W_value.min() > 0.99:\n",
    "                    break\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mvxGFFCA9LUm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arch: modified_MLP\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = scipy.io.loadmat('ks_simple.mat')\n",
    "# Test data\n",
    "usol = data['usol']\n",
    "\n",
    "\n",
    "# Hpyer-parameters\n",
    "key = random.PRNGKey(1234)\n",
    "M_t = 2\n",
    "M_x = 5\n",
    "t0 = 0.0\n",
    "t1 = 0.1\n",
    "n_t = 32\n",
    "n_x = 64\n",
    "tol_list = [1e-2, 1e-1, 1e0, 1e1, 1e2]\n",
    "layers = [256, 256, 256, 1] # using Fourier embedding so it is not 1\n",
    "\n",
    "# Initial state\n",
    "state0 = usol[:, 0:1]\n",
    "dt = 1 / 250\n",
    "idx = int(t1 / dt)\n",
    "t_star = data['t'][0][:idx]\n",
    "x_star = data['x'][0]\n",
    "\n",
    "# Create data set\n",
    "dataset = DataGenerator(t0, t1, n_t, n_x)\n",
    "\n",
    "arch = 'modified_MLP'\n",
    "print('arch:', arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5FBCv_cB-dS4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Time: 0.1\n",
      "tol: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 16000/200000 [06:28<1:14:24, 41.22it/s, Loss=0.018576013, loss_ics=1.0049725e-06, loss_res=0.01761831, W_min=0.99507654]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 22000/200000 [08:20<1:07:30, 43.94it/s, Loss=0.0022519836, loss_ics=5.4533274e-08, loss_res=0.0021981755, W_min=0.9936791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 21000/200000 [07:59<1:08:04, 43.82it/s, Loss=0.00027955463, loss_ics=1.5732489e-08, loss_res=0.000263852, W_min=0.99187297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 53000/200000 [20:10<55:57, 43.78it/s, Loss=3.250052e-05, loss_ics=1.0987131e-09, loss_res=3.1400064e-05, W_min=0.990752]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [1:16:10<00:00, 43.76it/s, Loss=6.7935566e-06, loss_ics=1.2075697e-10, loss_res=6.673555e-06, W_min=0.9792335]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative l2 error: 1.415e-05\n",
      "Final Time: 0.2\n",
      "tol: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 9000/200000 [03:40<1:18:01, 40.80it/s, Loss=0.026193082, loss_ics=4.2080796e-06, loss_res=0.022059413, W_min=0.9931115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 21000/200000 [07:32<1:04:20, 46.37it/s, Loss=0.0006027023, loss_ics=4.799636e-08, loss_res=0.0005546913, W_min=0.9983704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 20000/200000 [07:11<1:04:39, 46.40it/s, Loss=0.00031486352, loss_ics=5.7995482e-08, loss_res=0.00025691406, W_min=0.991971] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 78000/200000 [28:04<43:54, 46.31it/s, Loss=3.4828146e-05, loss_ics=2.075565e-09, loss_res=3.2737677e-05, W_min=0.9903438]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 180000/200000 [1:04:45<07:11, 46.32it/s, Loss=3.2523067e-06, loss_ics=1.6014348e-10, loss_res=3.0932e-06, W_min=0.99043393]   \n",
      "/home/siki/anaconda3/envs/jax/lib/python3.8/site-packages/numpy/lib/npyio.py:521: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative l2 error: 4.785e-05\n",
      "Final Time: 0.30000000000000004\n",
      "tol: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 12000/200000 [04:39<1:12:54, 42.98it/s, Loss=0.026212208, loss_ics=1.2680579e-06, loss_res=0.025040599, W_min=0.99267846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10000/200000 [03:30<1:06:39, 47.51it/s, Loss=0.0033600205, loss_ics=4.2919845e-07, loss_res=0.0029320414, W_min=0.9916128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 38000/200000 [13:19<56:48, 47.53it/s, Loss=0.0002764039, loss_ics=1.8270468e-08, loss_res=0.00025822863, W_min=0.9928121]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 39000/200000 [13:41<56:30, 47.49it/s, Loss=3.3861957e-05, loss_ics=1.0083993e-09, loss_res=3.286299e-05, W_min=0.990907]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 120426/200000 [42:15<27:55, 47.50it/s, Loss=6.819816e-06, loss_ics=1.9068919e-10, loss_res=6.6286916e-06, W_min=0.98113143]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtol:\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m.\u001b[39mtol)\n\u001b[1;32m     17\u001b[0m     \u001b[39m# Train\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     model\u001b[39m.\u001b[39;49mtrain(dataset, nIter\u001b[39m=\u001b[39;49m\u001b[39m200000\u001b[39;49m)\n\u001b[1;32m     20\u001b[0m \u001b[39m# Store\u001b[39;00m\n\u001b[1;32m     21\u001b[0m params \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_params(model\u001b[39m.\u001b[39mopt_state) \n",
      "Cell \u001b[0;32mIn[5], line 112\u001b[0m, in \u001b[0;36mPINN.train\u001b[0;34m(self, dataset, nIter)\u001b[0m\n\u001b[1;32m    110\u001b[0m pbar \u001b[39m=\u001b[39m trange(nIter)\n\u001b[1;32m    111\u001b[0m \u001b[39m# Main training loop\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m \u001b[39mfor\u001b[39;00m it \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m    113\u001b[0m     \u001b[39m# Get batch\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     batch\u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(res_data)\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_count \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitercount)\n",
      "File \u001b[0;32m~/anaconda3/envs/jax/lib/python3.8/site-packages/tqdm/std.py:1205\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1203\u001b[0m dt \u001b[39m=\u001b[39m cur_t \u001b[39m-\u001b[39m last_print_t\n\u001b[1;32m   1204\u001b[0m \u001b[39mif\u001b[39;00m dt \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m mininterval \u001b[39mand\u001b[39;00m cur_t \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m min_start_t:\n\u001b[0;32m-> 1205\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(n \u001b[39m-\u001b[39;49m last_print_n)\n\u001b[1;32m   1206\u001b[0m     last_print_n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_print_n\n\u001b[1;32m   1207\u001b[0m     last_print_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_print_t\n",
      "File \u001b[0;32m~/anaconda3/envs/jax/lib/python3.8/site-packages/tqdm/std.py:1256\u001b[0m, in \u001b[0;36mtqdm.update\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ema_dn(dn)\n\u001b[1;32m   1255\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ema_dt(dt)\n\u001b[0;32m-> 1256\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrefresh(lock_args\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlock_args)\n\u001b[1;32m   1257\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdynamic_miniters:\n\u001b[1;32m   1258\u001b[0m     \u001b[39m# If no `miniters` was specified, adjust automatically to the\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m     \u001b[39m# maximum iteration rate seen so far between two prints.\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m     \u001b[39m# e.g.: After running `tqdm.update(5)`, subsequent\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m     \u001b[39m# calls to `tqdm.update()` will only cause an update after\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m     \u001b[39m# at least 5 more iterations.\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxinterval \u001b[39mand\u001b[39;00m dt \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxinterval:\n",
      "File \u001b[0;32m~/anaconda3/envs/jax/lib/python3.8/site-packages/tqdm/std.py:1361\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1360\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39macquire()\n\u001b[0;32m-> 1361\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisplay()\n\u001b[1;32m   1362\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m nolock:\n\u001b[1;32m   1363\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/jax/lib/python3.8/site-packages/tqdm/std.py:1509\u001b[0m, in \u001b[0;36mtqdm.display\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[39mif\u001b[39;00m pos:\n\u001b[1;32m   1508\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoveto(pos)\n\u001b[0;32m-> 1509\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__str__\u001b[39;49m() \u001b[39mif\u001b[39;49;00m msg \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m msg)\n\u001b[1;32m   1510\u001b[0m \u001b[39mif\u001b[39;00m pos:\n\u001b[1;32m   1511\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoveto(\u001b[39m-\u001b[39mpos)\n",
      "File \u001b[0;32m~/anaconda3/envs/jax/lib/python3.8/site-packages/tqdm/std.py:350\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_status\u001b[39m(s):\n\u001b[1;32m    349\u001b[0m     len_s \u001b[39m=\u001b[39m disp_len(s)\n\u001b[0;32m--> 350\u001b[0m     fp_write(\u001b[39m'\u001b[39;49m\u001b[39m\\r\u001b[39;49;00m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m s \u001b[39m+\u001b[39;49m (\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39mmax\u001b[39;49m(last_len[\u001b[39m0\u001b[39;49m] \u001b[39m-\u001b[39;49m len_s, \u001b[39m0\u001b[39;49m)))\n\u001b[1;32m    351\u001b[0m     last_len[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m len_s\n",
      "File \u001b[0;32m~/anaconda3/envs/jax/lib/python3.8/site-packages/tqdm/std.py:344\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfp_write\u001b[39m(s):\n\u001b[1;32m    343\u001b[0m     fp\u001b[39m.\u001b[39mwrite(_unicode(s))\n\u001b[0;32m--> 344\u001b[0m     fp_flush()\n",
      "File \u001b[0;32m~/anaconda3/envs/jax/lib/python3.8/site-packages/tqdm/utils.py:145\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    146\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    147\u001b[0m         \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39merrno \u001b[39m!=\u001b[39m \u001b[39m5\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/jax/lib/python3.8/site-packages/ipykernel/iostream.py:488\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpub_thread\u001b[39m.\u001b[39mschedule(evt\u001b[39m.\u001b[39mset)\n\u001b[1;32m    487\u001b[0m     \u001b[39m# and give a timeout to avoid\u001b[39;00m\n\u001b[0;32m--> 488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m evt\u001b[39m.\u001b[39;49mwait(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflush_timeout):\n\u001b[1;32m    489\u001b[0m         \u001b[39m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[1;32m    490\u001b[0m         \u001b[39m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[1;32m    491\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIOStream.flush timed out\u001b[39m\u001b[39m\"\u001b[39m, file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39m__stderr__)\n\u001b[1;32m    492\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/jax/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    559\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/anaconda3/envs/jax/lib/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    307\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "u_pred_list = []\n",
    "params_list = []\n",
    "losses_list = []\n",
    "\n",
    "\n",
    "# Time marching\n",
    "for k in range(N):\n",
    "    # Initialize model\n",
    "    print('Final Time: {}'.format((k + 1) * t1))\n",
    "    model = PINN(key, arch, layers, M_x, state0, t0, t1, n_t, n_x)\n",
    "\n",
    "    # Train\n",
    "    for tol in tol_list:    \n",
    "        model.tol = tol\n",
    "        print(\"tol:\", model.tol)\n",
    "        # Train\n",
    "        model.train(dataset, nIter=200000)\n",
    "        \n",
    "    # Store\n",
    "    params = model.get_params(model.opt_state) \n",
    "    u_pred = model.u_pred_fn(params, t_star, x_star)\n",
    "    u_pred_list.append(u_pred)\n",
    "    flat_params, _  = ravel_pytree(params)\n",
    "    params_list.append(flat_params)\n",
    "    losses_list.append([model.loss_log, model.loss_ics_log, model.loss_res_log])\n",
    "    \n",
    "\n",
    "    np.save('u_pred_list.npy', u_pred_list)\n",
    "    np.save('params_list.npy', params_list)\n",
    "    np.save('losses_list.npy', losses_list)\n",
    "    \n",
    "    # error \n",
    "    u_preds = np.hstack(u_pred_list)\n",
    "    error = np.linalg.norm(u_preds - usol[:, :(k+1) * idx]) / np.linalg.norm(usol[:, :(k+1) * idx]) \n",
    "    print('Relative l2 error: {:.3e}'.format(error))\n",
    "    \n",
    "    params = model.get_params(model.opt_state)\n",
    "    u0_pred = vmap(model.neural_net, (None, None, 0))(params, t1, x_star)\n",
    "    state0 = u0_pred"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PINNs_Allen_Cahn_temporal_weighting.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
